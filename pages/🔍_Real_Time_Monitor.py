#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Real-Time Monitor - å®æ—¶ç›‘æ§é¡µé¢
ç›‘æ§åˆ†æè¿›ç¨‹çš„è¿è¡ŒçŠ¶æ€å’Œèµ„æºä½¿ç”¨æƒ…å†µ
"""

import streamlit as st
import psutil
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import time
import os
from pathlib import Path
import json
from datetime import datetime

st.set_page_config(
    page_title="Real-Time Monitor",
    page_icon="ğŸ”",
    layout="wide"
)

def get_system_info():
    """è·å–ç³»ç»Ÿä¿¡æ¯"""
    try:
        # è·å–CPUå’Œå†…å­˜ä¿¡æ¯
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()
        boot_time = datetime.fromtimestamp(psutil.boot_time())
        
        # å®‰å…¨åœ°è·å–ç£ç›˜ä¿¡æ¯
        disk_info = None
        if os.name == 'nt':
            # Windows: å°è¯•å¤šä¸ªå¸¸è§é©±åŠ¨å™¨
            for drive in ['C:', 'D:', 'E:', 'F:', 'G:']:
                try:
                    if os.path.exists(drive + os.sep):
                        disk_info = psutil.disk_usage(drive + os.sep)
                        break
                except:
                    continue
            
            # å¦‚æœéƒ½å¤±è´¥äº†ï¼Œå°è¯•å½“å‰ç›®å½•çš„é©±åŠ¨å™¨
            if disk_info is None:
                try:
                    current_drive = os.path.splitdrive(os.getcwd())[0]
                    if current_drive:
                        disk_info = psutil.disk_usage(current_drive + os.sep)
                except:
                    pass
        else:
            # Linux/Mac: ä½¿ç”¨æ ¹ç›®å½•
            try:
                disk_info = psutil.disk_usage('/')
            except:
                pass
        
        # å¦‚æœç£ç›˜ä¿¡æ¯è·å–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼
        if disk_info is None:
            disk_info = type('obj', (object,), {
                'total': 1024*1024*1024*100,  # 100GB
                'used': 1024*1024*1024*50,    # 50GB
                'free': 1024*1024*1024*50     # 50GB
            })()
        
        return {
            'cpu_percent': cpu_percent,
            'memory': memory,
            'disk': disk_info,
            'boot_time': boot_time
        }
        
    except Exception as e:
        st.error(f"è·å–ç³»ç»Ÿä¿¡æ¯å¤±è´¥: {e}")
        # è¿”å›å®‰å…¨çš„é»˜è®¤å€¼
        return {
            'cpu_percent': 0.0,
            'memory': type('obj', (object,), {
                'percent': 0, 
                'used': 0, 
                'total': 1024*1024*1024*8  # 8GB
            })(),
            'disk': type('obj', (object,), {
                'used': 0, 
                'total': 1024*1024*1024*100,  # 100GB
                'free': 1024*1024*1024*100    # 100GB
            })(),
            'boot_time': datetime.now()
        }

def find_python_processes():
    """æŸ¥æ‰¾Pythonç›¸å…³è¿›ç¨‹"""
    processes = []
    for proc in psutil.process_iter(['pid', 'name', 'cpu_percent', 'memory_info', 'cmdline']):
        try:
            if 'python' in proc.info['name'].lower():
                cmdline = ' '.join(proc.info['cmdline']) if proc.info['cmdline'] else ''
                if 'run.py' in cmdline or 'main.py' in cmdline or 'streamlit' in cmdline:
                    processes.append({
                        'pid': proc.info['pid'],
                        'name': proc.info['name'],
                        'cpu_percent': proc.info['cpu_percent'],
                        'memory_mb': proc.info['memory_info'].rss / 1024 / 1024,
                        'cmdline': cmdline[:100] + '...' if len(cmdline) > 100 else cmdline
                    })
        except (psutil.NoSuchProcess, psutil.AccessDenied):
            pass
    return processes

def monitor_log_files():
    """ç›‘æ§æ—¥å¿—æ–‡ä»¶å˜åŒ–"""
    log_files = []
    current_dir = Path('.')
    
    # æŸ¥æ‰¾æœ€è¿‘çš„æ—¥å¿—æ–‡ä»¶
    for log_file in current_dir.glob('*.log'):
        try:
            stat = log_file.stat()
            log_files.append({
                'name': log_file.name,
                'size_mb': stat.st_size / 1024 / 1024,
                'modified': datetime.fromtimestamp(stat.st_mtime),
                'path': str(log_file)
            })
        except:
            pass
    
    return sorted(log_files, key=lambda x: x['modified'], reverse=True)

def check_gpu_status():
    """æ£€æŸ¥GPUçŠ¶æ€"""
    # é¦–å…ˆå°è¯•ä½¿ç”¨nvidia-ml-pyè·å–çœŸå®çš„GPUä½¿ç”¨æƒ…å†µ
    try:
        import pynvml
        pynvml.nvmlInit()
        
        device_count = pynvml.nvmlDeviceGetCount()
        gpu_info = []
        
        for i in range(device_count):
            handle = pynvml.nvmlDeviceGetHandleByIndex(i)
            
            # è·å–åŸºæœ¬ä¿¡æ¯
            name = pynvml.nvmlDeviceGetName(handle).decode('utf-8')
            
            # è·å–å†…å­˜ä¿¡æ¯
            mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
            memory_total = mem_info.total / 1024**3  # GB
            memory_used = mem_info.used / 1024**3   # GB  
            memory_free = mem_info.free / 1024**3   # GB
            memory_utilization = (memory_used / memory_total) * 100
            
            # è·å–GPUä½¿ç”¨ç‡
            try:
                utilization = pynvml.nvmlDeviceGetUtilizationRates(handle)
                gpu_util = utilization.gpu
                memory_util = utilization.memory
            except:
                gpu_util = 0
                memory_util = 0
            
            # è·å–æ¸©åº¦
            try:
                temperature = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)
            except:
                temperature = 0
            
            # è·å–åŠŸè€—
            try:
                power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0  # W
                max_power = pynvml.nvmlDeviceGetMaxPowerManagement(handle) / 1000.0  # W
            except:
                power = 0
                max_power = 0
            
            gpu_info.append({
                'id': i,
                'name': name,
                'memory_total': memory_total,
                'memory_used': memory_used,
                'memory_free': memory_free,
                'memory_utilization': memory_utilization,
                'gpu_utilization': gpu_util,
                'memory_bandwidth_util': memory_util,
                'temperature': temperature,
                'power_usage': power,
                'max_power': max_power,
                'power_utilization': (power / max_power * 100) if max_power > 0 else 0
            })
        
        return gpu_info
        
    except ImportError:
        # å¦‚æœæ²¡æœ‰pynvmlï¼Œå°è¯•ä½¿ç”¨PyTorchï¼ˆåŠŸèƒ½æœ‰é™ï¼‰
        try:
            import torch
            if torch.cuda.is_available():
                gpu_count = torch.cuda.device_count()
                gpu_info = []
                for i in range(gpu_count):
                    try:
                        # å°è¯•è·å–è®¾å¤‡å±æ€§ï¼Œè¿™å¯èƒ½ä¼šå¤±è´¥
                        props = torch.cuda.get_device_properties(i)
                        memory_allocated = torch.cuda.memory_allocated(i) / 1024**3  # GB
                        memory_cached = torch.cuda.memory_reserved(i) / 1024**3  # GB
                        memory_total = props.total_memory / 1024**3  # GB
                        
                        gpu_info.append({
                            'id': i,
                            'name': props.name,
                            'memory_total': memory_total,
                            'memory_used': memory_allocated,  # PyTorchåˆ†é…çš„å†…å­˜
                            'memory_free': memory_total - memory_cached,
                            'memory_utilization': (memory_allocated / memory_total) * 100,
                            'gpu_utilization': 0,  # PyTorchæ— æ³•è·å–
                            'memory_bandwidth_util': 0,
                            'temperature': 0,
                            'power_usage': 0,
                            'max_power': 0,
                            'power_utilization': 0,
                            'pytorch_only': True  # æ ‡è®°è¿™æ˜¯PyTorché™åˆ¶ç‰ˆæœ¬
                        })
                    except Exception as gpu_err:
                        # å¦‚æœè·å–ç‰¹å®šGPUä¿¡æ¯å¤±è´¥ï¼Œåˆ›å»ºä¸€ä¸ªåŸºç¡€æ¡ç›®
                        gpu_info.append({
                            'id': i,
                            'name': f'GPU {i} (ä¿¡æ¯è·å–å¤±è´¥)',
                            'memory_total': 1.0,
                            'memory_used': 0.0,
                            'memory_free': 1.0,
                            'memory_utilization': 0.0,
                            'gpu_utilization': 0,
                            'memory_bandwidth_util': 0,
                            'temperature': 0,
                            'power_usage': 0,
                            'max_power': 0,
                            'power_utilization': 0,
                            'error': str(gpu_err)
                        })
                return gpu_info
            else:
                return []
        except ImportError:
            return None
    except Exception as e:
        # æ•è·æ‰€æœ‰å…¶ä»–é”™è¯¯
        return {'error': f'GPUæ£€æŸ¥å¤±è´¥: {str(e)}'}

def main():
    st.title("ğŸ” Real-Time System Monitor")
    st.markdown("å®æ—¶ç›‘æ§ç³»ç»Ÿèµ„æºå’Œåˆ†æè¿›ç¨‹çŠ¶æ€")
    
    # åˆ›å»ºæ›´æ–°æ§åˆ¶
    auto_refresh = st.sidebar.checkbox("è‡ªåŠ¨åˆ·æ–°", value=False)
    refresh_interval = st.sidebar.slider("åˆ·æ–°é—´éš”(ç§’)", 1, 10, 3)
    
    if st.sidebar.button("ğŸ”„ ç«‹å³åˆ·æ–°"):
        st.rerun()
    
    # å¦‚æœå¯ç”¨è‡ªåŠ¨åˆ·æ–°ï¼Œè®¾ç½®å®šæ—¶å™¨
    if auto_refresh:
        time.sleep(refresh_interval)
        st.rerun()
    
    # ç³»ç»Ÿèµ„æºçŠ¶æ€
    st.subheader("ğŸ’» ç³»ç»Ÿèµ„æºçŠ¶æ€")
    
    system_info = get_system_info()
    
    # åˆ›å»ºæŒ‡æ ‡è¡Œ
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        cpu_color = "normal"
        if system_info['cpu_percent'] > 80:
            cpu_color = "inverse"
        st.metric(
            "CPUä½¿ç”¨ç‡", 
            f"{system_info['cpu_percent']:.1f}%",
            delta=f"{'âš ï¸ é«˜è´Ÿè½½' if system_info['cpu_percent'] > 80 else 'âœ… æ­£å¸¸'}"
        )
    
    with col2:
        memory = system_info['memory']
        memory_percent = memory.percent
        st.metric(
            "å†…å­˜ä½¿ç”¨ç‡",
            f"{memory_percent:.1f}%",
            delta=f"{memory.used // 1024**3:.1f}GB / {memory.total // 1024**3:.1f}GB"
        )
    
    with col3:
        disk = system_info['disk']
        disk_percent = (disk.used / disk.total) * 100
        st.metric(
            "ç£ç›˜ä½¿ç”¨ç‡",
            f"{disk_percent:.1f}%",
            delta=f"{disk.free // 1024**3:.1f}GB å¯ç”¨"
        )
    
    with col4:
        uptime = datetime.now() - system_info['boot_time']
        st.metric(
            "ç³»ç»Ÿè¿è¡Œæ—¶é—´",
            f"{uptime.days}å¤©",
            delta=f"{uptime.seconds // 3600}å°æ—¶"
        )
    
    # ç³»ç»Ÿèµ„æºå›¾è¡¨
    if st.checkbox("æ˜¾ç¤ºèµ„æºè¶‹åŠ¿å›¾"):
        # è¿™é‡Œå¯ä»¥æ·»åŠ å†å²æ•°æ®æ”¶é›†å’Œè¶‹åŠ¿å›¾
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=['CPUä½¿ç”¨ç‡', 'å†…å­˜ä½¿ç”¨ç‡', 'ç£ç›˜ä½¿ç”¨ç‡', 'ç³»ç»Ÿè´Ÿè½½'],
            specs=[[{"secondary_y": False}, {"secondary_y": False}],
                   [{"secondary_y": False}, {"secondary_y": False}]]
        )
        
        # æ¨¡æ‹Ÿæ•°æ®ç‚¹
        x_data = list(range(10))
        fig.add_trace(
            go.Scatter(x=x_data, y=[system_info['cpu_percent']] * 10, 
                      name='CPU', line=dict(color='red')),
            row=1, col=1
        )
        fig.add_trace(
            go.Scatter(x=x_data, y=[memory_percent] * 10, 
                      name='Memory', line=dict(color='blue')),
            row=1, col=2
        )
        
        fig.update_layout(height=400, showlegend=False)
        st.plotly_chart(fig, width='stretch')
    
    # Pythonè¿›ç¨‹ç›‘æ§
    st.subheader("ğŸ Pythonè¿›ç¨‹ç›‘æ§")
    
    processes = find_python_processes()
    
    if processes:
        import pandas as pd
        df = pd.DataFrame(processes)
        
        # è¿›ç¨‹ç»Ÿè®¡
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("æ´»è·ƒPythonè¿›ç¨‹", len(processes))
        with col2:
            total_cpu = sum(p['cpu_percent'] for p in processes)
            st.metric("æ€»CPUä½¿ç”¨", f"{total_cpu:.1f}%")
        with col3:
            total_memory = sum(p['memory_mb'] for p in processes)
            st.metric("æ€»å†…å­˜ä½¿ç”¨", f"{total_memory:.1f}MB")
        
        # è¿›ç¨‹è¯¦æƒ…è¡¨
        st.dataframe(
            df.style.format({
                'cpu_percent': '{:.1f}%',
                'memory_mb': '{:.1f}MB'
            }),
            width='stretch'
        )
        
        # å¦‚æœå‘ç°è¿è¡Œåˆ†æçš„è¿›ç¨‹
        analysis_processes = [p for p in processes if 'run.py' in p['cmdline']]
        if analysis_processes:
            st.success(f"ğŸŸ¢ æ£€æµ‹åˆ° {len(analysis_processes)} ä¸ªåˆ†æè¿›ç¨‹æ­£åœ¨è¿è¡Œ")
            for proc in analysis_processes:
                with st.expander(f"è¿›ç¨‹ {proc['pid']} è¯¦æƒ…"):
                    st.code(proc['cmdline'])
                    st.text(f"CPU: {proc['cpu_percent']:.1f}% | å†…å­˜: {proc['memory_mb']:.1f}MB")
    else:
        st.info("æ²¡æœ‰æ£€æµ‹åˆ°ç›¸å…³çš„Pythonè¿›ç¨‹")
    
    # GPUçŠ¶æ€ç›‘æ§
    st.subheader("ğŸ® GPUçŠ¶æ€ç›‘æ§")
    
    gpu_info = check_gpu_status()
    
    if gpu_info is None:
        st.warning("PyTorchå’ŒNVIDIA-ML-PYéƒ½æœªå®‰è£…ï¼Œæ— æ³•æ£€æµ‹GPUçŠ¶æ€")
        st.info("å®‰è£…å‘½ä»¤: pip install nvidia-ml-py æˆ– conda install nvidia-ml-py")
    elif isinstance(gpu_info, dict) and 'error' in gpu_info:
        st.error(f"GPUçŠ¶æ€æ£€æŸ¥å¤±è´¥: {gpu_info['error']}")
        st.info("å»ºè®®å®‰è£…nvidia-ml-pyä»¥è·å¾—å®Œæ•´GPUç›‘æ§åŠŸèƒ½")
    elif not gpu_info:
        st.info("æœªæ£€æµ‹åˆ°å¯ç”¨çš„CUDA GPU")
    else:
        for gpu in gpu_info:
            with st.expander(f"GPU {gpu['id']}: {gpu['name']}", expanded=True):
                if 'error' in gpu:
                    st.error(f"GPU {gpu['id']} ä¿¡æ¯è·å–å¤±è´¥: {gpu['error']}")
                    st.info("å°è¯•é‡å¯åº”ç”¨ç¨‹åºæˆ–æ£€æŸ¥CUDAé©±åŠ¨ç‰ˆæœ¬")
                else:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯åŠŸèƒ½é™åˆ¶ç‰ˆæœ¬
                    is_pytorch_only = gpu.get('pytorch_only', False)
                    if is_pytorch_only:
                        st.info("âš ï¸ ä½¿ç”¨PyTorchç›‘æ§(åŠŸèƒ½æœ‰é™) - å»ºè®®å®‰è£…nvidia-ml-pyä»¥è·å¾—å®Œæ•´ç›‘æ§")
                    
                    # ä¸»è¦æŒ‡æ ‡
                    col1, col2, col3, col4 = st.columns(4)
                    
                    with col1:
                        st.metric("æ˜¾å­˜ä½¿ç”¨ç‡", f"{gpu['memory_utilization']:.1f}%")
                    with col2:
                        st.metric("å·²ç”¨æ˜¾å­˜", f"{gpu['memory_used']:.2f}GB")
                    with col3:
                        st.metric("æ€»æ˜¾å­˜", f"{gpu['memory_total']:.2f}GB")
                    with col4:
                        if gpu['gpu_utilization'] > 0:
                            st.metric("GPUä½¿ç”¨ç‡", f"{gpu['gpu_utilization']:.1f}%")
                        else:
                            st.metric("GPUä½¿ç”¨ç‡", "N/A")
                    
                    # é¢å¤–ä¿¡æ¯(å¦‚æœå¯ç”¨)
                    if not is_pytorch_only:
                        col5, col6, col7 = st.columns(3)
                        with col5:
                            if gpu['temperature'] > 0:
                                temp_color = "ğŸ”¥" if gpu['temperature'] > 80 else "ğŸŒ¡ï¸"
                                st.metric("æ¸©åº¦", f"{temp_color} {gpu['temperature']}Â°C")
                            else:
                                st.metric("æ¸©åº¦", "N/A")
                        with col6:
                            if gpu['power_usage'] > 0:
                                st.metric("åŠŸè€—", f"âš¡ {gpu['power_usage']:.1f}W")
                            else:
                                st.metric("åŠŸè€—", "N/A")
                        with col7:
                            if gpu['memory_bandwidth_util'] > 0:
                                st.metric("å†…å­˜å¸¦å®½", f"{gpu['memory_bandwidth_util']:.1f}%")
                            else:
                                st.metric("å†…å­˜å¸¦å®½", "N/A")
                    
                    # æ˜¾å­˜ä½¿ç”¨å›¾è¡¨
                    try:
                        fig = go.Figure()
                        
                        # æ˜¾å­˜ä½¿ç”¨æ¡å½¢å›¾
                        fig.add_trace(go.Bar(
                            x=['å·²ä½¿ç”¨', 'ç©ºé—²'],
                            y=[gpu['memory_used'], gpu['memory_free']],
                            marker_color=['#ff6b6b', '#4ecdc4'],
                            name='æ˜¾å­˜çŠ¶æ€'
                        ))
                        
                        fig.update_layout(
                            title=f"GPU {gpu['id']} æ˜¾å­˜ä½¿ç”¨æƒ…å†µ",
                            yaxis_title="æ˜¾å­˜ (GB)",
                            height=300,
                            showlegend=False
                        )
                        
                        st.plotly_chart(fig, width='stretch')
                        
                        # å¦‚æœæœ‰GPUä½¿ç”¨ç‡ä¿¡æ¯ï¼Œæ·»åŠ ä½¿ç”¨ç‡å›¾è¡¨
                        if not is_pytorch_only and gpu['gpu_utilization'] > 0:
                            fig2 = go.Figure()
                            
                            # åˆ›å»ºä»ªè¡¨ç›˜æ ·å¼çš„å›¾è¡¨
                            fig2.add_trace(go.Indicator(
                                mode="gauge+number+delta",
                                value=gpu['gpu_utilization'],
                                domain={'x': [0, 0.5], 'y': [0, 1]},
                                title={'text': "GPUä½¿ç”¨ç‡"},
                                gauge={'axis': {'range': [None, 100]},
                                       'bar': {'color': "#ff6b6b"},
                                       'steps': [
                                           {'range': [0, 50], 'color': "#4ecdc4"},
                                           {'range': [50, 80], 'color': "#ffd93d"},
                                           {'range': [80, 100], 'color': "#ff6b6b"}],
                                       'threshold': {'line': {'color': "red", 'width': 4},
                                                     'thickness': 0.75, 'value': 90}}
                            ))
                            
                            fig2.add_trace(go.Indicator(
                                mode="gauge+number",
                                value=gpu['memory_utilization'],
                                domain={'x': [0.5, 1], 'y': [0, 1]},
                                title={'text': "æ˜¾å­˜ä½¿ç”¨ç‡"},
                                gauge={'axis': {'range': [None, 100]},
                                       'bar': {'color': "#6c5ce7"},
                                       'steps': [
                                           {'range': [0, 50], 'color': "#4ecdc4"},
                                           {'range': [50, 80], 'color': "#ffd93d"},
                                           {'range': [80, 100], 'color': "#ff6b6b"}]}
                            ))
                            
                            fig2.update_layout(height=250, margin={'t': 50, 'b': 0, 'l': 0, 'r': 0})
                            st.plotly_chart(fig2, width='stretch')
                            
                    except Exception as plot_err:
                        st.warning(f"å›¾è¡¨æ˜¾ç¤ºå¤±è´¥: {plot_err}")
    
    # æ—¥å¿—æ–‡ä»¶ç›‘æ§
    st.subheader("ğŸ“‹ æ—¥å¿—æ–‡ä»¶ç›‘æ§")
    
    log_files = monitor_log_files()
    
    if log_files:
        for log_file in log_files[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
            with st.expander(f"ğŸ“„ {log_file['name']} ({log_file['size_mb']:.2f}MB)"):
                st.text(f"æœ€åä¿®æ”¹: {log_file['modified']}")
                st.text(f"æ–‡ä»¶å¤§å°: {log_file['size_mb']:.2f}MB")
                
                # æ˜¾ç¤ºæœ€åå‡ è¡Œ
                try:
                    with open(log_file['path'], 'r', encoding='utf-8') as f:
                        lines = f.readlines()
                        if lines:
                            st.text_area(
                                "æœ€å10è¡Œ:",
                                ''.join(lines[-10:]),
                                height=200,
                                disabled=True
                            )
                except:
                    st.error("æ— æ³•è¯»å–æ—¥å¿—æ–‡ä»¶")
    else:
        st.info("å½“å‰ç›®å½•ä¸‹æ²¡æœ‰å‘ç°æ—¥å¿—æ–‡ä»¶")

if __name__ == "__main__":
    main()